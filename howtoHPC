how to HPC : for fergal

------------------------------------------------------------------------------

SETUP

1. Set up your virtual environment
In order to run stuff on the HPC, it helps if you have a virtual environment already set up. 
Here are the instructions:
https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html

a. open up a terminal in your home directory
b. run anaconda-setup if you havent (i think you just type anaconda-setup in)
c. I think anaconda then runs & gets set up - double check it has, if its been successful, you should have a new folder called anaconda3 in your home directory
d. Anaconda is now installed! Fabulous. This is essential for you to make a virtual environment. 
e. to make a virtual environment:
[note!! idk if this line is strictly necessary]:
module load anaconda3/personal 
[i think it loads the place you wanna keep ur environments]
[this creates an environment, I choose R-env as the environment name]
conda create --name R-env
[activate your new R environment!]
conda activate R-env

What should pop up is something like
(R-env) bash-4.2$ in ur terminal, now that its activated

2. Cool, now you've got a virtual environment! You now need to install packages.
While the environment is activated, you must type 
conda install r-pheatmap
Syntax: r-<package-name>
You need to install all the packages your scripts use in your virtual environment, or else your scripts will bug out

Some packages are legacy packages or user defined.
For user defined ones (like penalisation_functions.R from barbara), you can include a line in your script like:
source(penalisation_functions.R) if you have the R script loaded into the directory your script is working from (you can also specify the path elsewhere)
For legacy packages (like QUIC, that were retired for later versions of R), GOOGLE how to install.
Sometimes it's weird, like for quick you have to do:
conda install -c r_test r-quic

3. YAML file
I'm gonna be honest. I don't really know what this does or if its necessary. But if you get errors when submitting jobs, follow these instructions & write a YAML file just in case.

I get the sense that YAML files are just other ways of communicating your environment to your computer.
You make one by:
a. going to the envs folder (or personal folder) within your anaconda3 folder in your home directory
b. open a text file

Copy & paste:
name: R-env
channels:
  - defaults
  - bioconda
  - conda-forge
  - imperial-college-research-computing
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_gnu
  - libgomp=9.3.0=h2828fa1_18
prefix: /rds/general/user/YOURUSERNAME/home/anaconda3/envs/R-env

This was copy & pasted from Nina's presentation, and Malo helped me with the last sentence (idk what it means or does but it works so am not questioning it).

Save the file with a .yml suffix. 
That's it!

------------------------------------------------------------------------------

JOBS

Cool, now you've basically set up an environment for your scripts to run in, so they can have access to all the packages they need and not bog down your environment.


now it's time for jobs!

There are two types:
- normal ones
- array ones 
See Week 2 & Week 4 of TDS for more info. The html files are good for this. 

Array jobs run on multiple computers at once, and I think also iterate through your script completely for each session.
Basically they are for loops for jobs, so you can run same job for multiple iterations of a value.

In order to run a job, you need to 
1. Write a bash script (tells the HPC what to do)

more details on bash script parameters here:
https://imperialcollegelondon.app.box.com/s/kwjxbd5bc87w296wo0m7fdwo9jct5vvs

a. create a new shell script file

copy & paste:

TEMPLATE for normal job:
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=1:mem=2gb

module load anaconda3/personal
source activate R-env
cd /rds/general/project/hda_students_data/live/Group3/TDS_Group3/Scripts/

Rscript script.R 

TEMPLATE for array job:
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=2:mem=20gb
#PBS -J 1-4

module load anaconda3/personal
source activate R-env
cd /rds/general/project/hda_students_data/live/Group3/TDS_Group3/Scripts/nvsm

m=$PBS_ARRAY_INDEX # model index

Rscript nvsm_spls.R $m

The differences here are the parameters (ncpus, memory, and the weird PBS J thing), as well as the array stuff.
Ncpus & memory is pretty straightforward - thats just the number of computers you want to run your job on and the memory you want to reserve to run it. Higher values for sPLS, lower values for lasso - kind of corresponds as well to the length of time your job takes. This is pretty much trial and error to figure out.

Array stuff - 
#PBS J 1-4
^ this is number of times you want to iterate through your script
For the townsend scripts, you had 12 models to run, so you'll see in the bash script that I wrote #PBS -J 1-12
These are the numbers that are passed to m, the model index. 
In your array job, you'll notice that m is called by the rscript. 

This is where it was called in strat_townsend_spls:
## Parameters
args=commandArgs(trailingOnly=TRUE)
m=as.numeric(args[1])

## Load data set
arr=paste0(rep(c("lung","bladder"),each=2),rep(c("low","mid",'high'),each=4),".",1:2)[m] ### Change name for other stratification

So for every iteration, you specify which model.

2. Send the bash script to the HPC through the terminal

Cool, now you have
a. an R script
b. a bash script
c. a virtual environment

You are good to go!

Send your job to the queue by typing out
qsub <path to bashscript>
if its in your home directory or you've changed directories to where your script is, you only need the name, not the path
if its in a subfolder, you can type something like
qsub /rds/general/project/hda_students_data/live/Group3/TDS_Group3/Scripts/subtype/sh/subtype_lasso.sh

You'll get a job id you can check up on through qstat <jobID>

-------------------------------------------------------------------------------

ERRORS

Cool, now you've submitted stuff. What's gonna happen?

After you've written & submitted your job, what you're most often gonna be left with is debugging.

You'll get a bunch of new files popping up in your home directory.
Some of them will be .o files (which seem to go over the operation of the script)
& some will be .e files (which are error files)
They will be updated as you iterate, and for array jobs you'll get the same number of files as you do for iterations.
This is where you will get your error messages.

These scripts are useful to check if what you did actually work.

1. Check .e scripts
2. check your bash scripts + r scripts for glaring mistakes
3. check you've installed all packages on your virtual environment
4. google your error messages
5. reach out colleagues

A lot of the times stuff that I ran into was solved by Rin 3 weeks ago (like the fact that CalibrateRegression doesn't like dataframes, so you have to pass a matrix to it, or that you have to remove all NAs to have a good model), so don't hesitate.

6. last resort: go through script line by line pretending you're on some random iteration, and check out the error messages as they crop up + experiment with solutions
^ this is the worst case scenario because
a) its boring
b) it takes up a lot of memory on the server
